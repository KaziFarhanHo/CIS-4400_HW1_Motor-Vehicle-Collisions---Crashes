# -*- coding: utf-8 -*-
"""CIS4400_Homework_Nevessary_Codes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xWnsQ6go1glRsLBELzI49apl4HJTwKgc
"""

#-------For getting the data-------

import requests

# API endpoint URL
url = 'https://data.cityofnewyork.us/resource/h9gi-nx95.json'

response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Convert the response to JSON format
    data = response.json()

    # Print the data
    print(data)
else:
    print(f'Failed to retrieve data. Status code: {response.status_code}')

! pip install azure-storage-blob

#-------Important Libraries-------

import pandas as pd
import numpy as np
import json
import requests
from io import StringIO
from azure. storage. blob import BlobServiceClient, BlobClient, ContainerClient
!pip install pyarrow

#-------For fetching the fist 8000 rows of data-------

import pandas as pd
import requests

# Initialize variables
URL = "https://data.cityofnewyork.us/resource/h9gi-nx95.json"
LIMIT = 1000  # Set the limit for each request, can be up to 50,000 for SODA 2.0 endpoints
total_rows_to_fetch = 8000  # Total rows you want to fetch
rows_fetched = 0  # Keep track of the number of rows fetched
all_data = []  # List to store all the fetched data

# Loop to fetch data in batches
while rows_fetched < total_rows_to_fetch:
    # Calculate the number of rows to fetch in this iteration
    rows_to_fetch = min(LIMIT, total_rows_to_fetch - rows_fetched)

    # Append the limit and offset parameters to the URL
    url_with_params = f"{URL}?$limit={rows_to_fetch}&$offset={rows_fetched}"
    response = requests.get(url_with_params)

    # Check if the response contains data
    if response.status_code == 200:
        data = response.json()
        if not data:  # If no data is returned, break the loop
            break
        all_data.extend(data)
        rows_fetched += len(data)  # Update the count of rows fetched
    else:
        print(f"Failed to fetch data: {response.status_code}")
        break

# Convert the list of data to a DataFrame
df_raw = pd.DataFrame(all_data)

# Display DataFrame information and the first few rows
print(df_raw.info())
print(df_raw.shape)
df_raw.head()

#------For Loading the first 8000 rows of data on MS Azure-------

from azure.storage.blob import BlobServiceClient
from io import StringIO
import pandas as pd

# Connection string and container/blob details
CONNECTION_STRING_AZURE_STORAGE = "DefaultEndpointsProtocol=https;AccountName=cis4400kfh;AccountKey=dgKsJG9pVgtKeRiSaSKaRhv/lwylyyOL4wKDKFC9vC4Q4qN6ZTecnwFhCN2ck0uaMuuDakpnugYH+ASt+xAE3Q==;EndpointSuffix=core.windows.net"
CONTAINER_AZURE = 'vehiclecollisions'
blob_name = "vehiclecollision.csv"

# Assuming df_raw is your DataFrame
# df_raw = your_dataframe_here

# Convert DataFrame to CSV format using StringIO
output = StringIO()
df_raw.to_csv(output, index=False)
data = output.getvalue()
output.close()

# Create the BlobServiceClient object
blob_service_client = BlobServiceClient.from_connection_string(CONNECTION_STRING_AZURE_STORAGE)

# Get a blob client using the container name and blob name
blob_client = blob_service_client.get_blob_client(container=CONTAINER_AZURE, blob=blob_name)

# Upload the CSV data
blob_client.upload_blob(data, overwrite=True)

print(f"Uploaded {blob_name} to Azure Blob Storage in container {CONTAINER_AZURE}.")

